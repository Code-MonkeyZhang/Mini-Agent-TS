import * as path from "node:path";
import * as fs from "node:fs";
import { LLMClient } from "./llm/llm_wrapper.js";
import type { Message, ToolCall } from "./schema/index.js";

// ============ Â∏∏Èáè ============

const SEPARATOR_WIDTH = 60;

// ============ ËæÖÂä©ÂáΩÊï∞ ============

function buildSystemPrompt(basePrompt: string, workspaceDir: string): string {
  if (basePrompt.includes("Current Workspace")) {
    return basePrompt;
  }
  return (
    basePrompt +
    `

## Current Workspace
You are currently working in: \`${workspaceDir}\`
All relative paths will be resolved relative to this directory.`
  );
}

// ============ Agent Á±ª ============

export class Agent {
  public llmClient: LLMClient;
  public systemPrompt: string;
  public maxSteps: number;
  public messages: Message[];
  public tokenLimit: number;
  public workspaceDir: string;

  constructor(
    llmClient: LLMClient,
    systemPrompt: string,
    maxSteps: number = 50,
    workspaceDir: string = "./workspace",
    tokenLimit: number = 8000
  ) {
    this.llmClient = llmClient;
    this.maxSteps = maxSteps;
    this.tokenLimit = tokenLimit;

    // Ensure workspace exists
    this.workspaceDir = path.resolve(workspaceDir);
    fs.mkdirSync(this.workspaceDir, { recursive: true });

    // Â∞Ü workspace dir Ê≥®ÂÖ• system prompt
    this.systemPrompt = buildSystemPrompt(systemPrompt, workspaceDir);
    this.messages = [{ role: "system", content: this.systemPrompt }];

    // TODO: ÂàùÂßãÂåñ Logger
    // TODO: ÂêØÂä® TOKEN ËÆ°ÁÆó
  }

  addUserMessage(content: string): void {
    this.messages.push({ role: "user", content });
  }

  clearHistoryKeepSystem(): number {
    const removed = this.messages.length - 1;
    this.messages = [this.messages[0]];
    return removed;
  }

  async run(): Promise<string> {
    for (let step = 0; step < this.maxSteps; step++) {
      // TODO: Check and summarize message history to prevent context overflow

      // ÊâìÂç∞ header
      console.log();
      console.log("ü§ñ Assistant:");

      // ÊµÅÂºèËæìÂá∫
      let fullContent = "";
      let fullThinking = "";
      let toolCalls: ToolCall[] | null = null;
      let isThinkingPrinted = false;

      for await (const chunk of this.llmClient.generateStream(this.messages)) {
        // ÊâìÂç∞ÊÄùËÄÉÂÜÖÂÆπ
        if (chunk.thinking) {
          if (!isThinkingPrinted) {
            console.log("üí≠ Thinking:");
            console.log("‚îÄ".repeat(SEPARATOR_WIDTH));
            isThinkingPrinted = true;
          }
          process.stdout.write(chunk.thinking);
          fullThinking += chunk.thinking;
        }

        // ÊâìÂç∞‰∏ªÂÜÖÂÆπ
        if (chunk.content) {
          if (isThinkingPrinted && fullContent === "") {
            // ÊÄùËÄÉÁªìÊùüÔºåÂºÄÂßãËæìÂá∫ÂÜÖÂÆπ
            console.log();
            console.log("‚îÄ".repeat(SEPARATOR_WIDTH));
            console.log();
          }
          process.stdout.write(chunk.content);
          fullContent += chunk.content;
        }

        // Êî∂ÈõÜ tool calls
        if (chunk.tool_calls) {
          toolCalls = chunk.tool_calls;
        }
      }

      // Êç¢Ë°å
      console.log();

      // Add assistant message
      this.messages.push({
        role: "assistant",
        content: fullContent,
        thinking: fullThinking || null,
        tool_calls: toolCalls,
      });

      // Check if task is complete (no tool calls)
      if (!toolCalls || toolCalls.length === 0) {
        return fullContent;
      }

      // TODO: Execute tool calls
    }

    return `Task couldn't be completed after ${this.maxSteps} steps.`;
  }
}
